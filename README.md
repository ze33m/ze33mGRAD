# ze33mGRAD

Мой pet-проект, представляющий собой минималистичную реализацию системы автоматического дифференцирования (autograd) с нуля.

## Цели проекта

Проект создан для глубокого понимания внутреннего устройства PyTorch, TensorFlow и подобных фреймворков машинного обучения. Основная задача — изучить математические основы автоматического дифференцирования через практическую реализацию алгоритмов обратного распространения.

## Что реализовано

Класс Tensor поддерживает основные матричные операции: матричное умножение через оператор `@`, поэлементное сложение и вычитание, умножение на скаляр. Каждая операция автоматически строит граф вычислений, отслеживая зависимости между тензорами и сохраняя информацию, необходимую для обратного распространения градиентов. Реализована функция `sum()` для агрегирования элементов тензора в скалярное значение, что позволяет создавать функции потерь.

Система автоматического дифференцирования работает через построение направленного ациклического графа операций, где каждый узел представляет тензор, а рёбра — операции между ними. При вызове метода `backward()` выполняется топологическая сортировка графа и обратный проход с вычислением градиентов для всех участвующих в вычислениях тензоров.

## Ноутбуки

Into Tensors.ipynb демонстрирует работу системы на примере матричного умножения с полным математическим выводом градиентов. В ноутбуке показано построение и визуализация графа вычислений, аналитический вывод формул для градиентов операции матричного умножения и их численная проверка. Планируется создание серии ноутбуков, покрывающих различные аспекты автоматического дифференцирования.

## Пример использования

```python
from engine.Tensor import Tensor

# Создание тензоров
A = Tensor([[1, 2, 4], [4, 6, 8], [2, 9, 1]])
B = Tensor([[1], [432], [45]])

# Прямой проход
C = A @ B
S = C.sum()  # Скалярная функция потерь

# Обратный проход
S.backward()

# Градиенты доступны в A.grad и B.grad
print("Gradient of A:")
print(A.grad)
```
